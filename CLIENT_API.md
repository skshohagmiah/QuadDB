# GoMsg Client API Reference

**Complete list of all client APIs based on actual server implementations.**

## ðŸ“Š KV APIs (12 methods)

**Basic Operations:**
- `Set` - Store key-value with optional TTL
- `Get` - Retrieve value by key  
- `Del` - Delete one or more keys
- `Exists` - Check if key exists
- `Expire` - Set TTL for existing key
- `TTL` - Get remaining time-to-live

**Batch Operations:**
- `MSet` - Set multiple key-value pairs
- `MGet` - Get multiple values

**Atomic Operations:**
- `Incr` - Increment counter (with optional `by` parameter)
- `Decr` - Decrement counter (with optional `by` parameter)

**Pattern Operations:**
- `Keys` - Find keys matching pattern (with limit)
- `DelPattern` - Delete keys matching pattern

## ðŸ“¬ Queue APIs (10 methods)

**Basic Operations:**
- `Push` - Add message to queue (with optional delay)
- `Pop` - Remove and return message (with timeout)
- `Peek` - View messages without removing (with limit)
- `Ack` - Acknowledge message processing (by messageId)
- `Nack` - Negative acknowledgment (by messageId)

**Batch Operations:**
- `PushBatch` - Add multiple messages with delays
- `PopBatch` - Remove multiple messages (with limit and timeout)

**Management:**
- `Stats` - Get queue statistics
- `Purge` - Remove all messages
- `Delete` - Remove entire queue
- `List` - Get all queue names

**Note:** ACK/NACK work on message IDs, not message objects

## ðŸŒŠ Stream APIs (13 methods)

**Publishing:**
- `Publish` - Send message to stream

**Reading:**
- `Read` - Read messages from offset (with partition, limit)
- `ReadFrom` - Read messages from timestamp
- `Subscribe` - Real-time streaming (gRPC stream)

**Topic Management:**
- `CreateTopic` - Create topic with partitions
- `DeleteTopic` - Remove topic
- `ListTopics` - Get all topics
- `GetTopicInfo` - Get topic details and partition info
- `Purge` - Remove all messages from topic

**Consumer Management:**
- `Seek` - Set offset for consumer
- `GetOffset` - Get current consumer offset
- `SubscribeGroup` - Join consumer group (gRPC stream)
- `CommitGroupOffset` - Commit offset for consumer group
- `GetGroupOffset` - Get current group offset

## ðŸ”§ Connection APIs (3 methods)

- `Connect` - Connect to cluster
- `Health` - Check node health
- `Close` - Close all connections

---

## ðŸ“‹ Complete API Summary

| **Service** | **Method Count** | **Total APIs** |
|-------------|------------------|----------------|
| **KV** | 12 methods | **12** |
| **Queue** | 10 methods | **10** |
| **Stream** | 13 methods | **13** |
| **Connection** | 3 methods | **3** |
| **TOTAL** | | **38 APIs** |

## ðŸŽ¯ Key Notes

**Actual Method Names:**
- KV: `Incr/Decr` (not `Increment/Decrement`)
- Queue: `Del` (not `Delete` for keys)
- Stream: Consumer groups with `GroupId/ConsumerId`

**Parameters:**
- `Incr/Decr` support optional `by` parameter
- `Keys` supports `limit` parameter
- `Peek/PopBatch` support `limit` parameter
- All operations include proper timeout handling

**Enterprise Features:**
- Message acknowledgment (ACK/NACK)
- Consumer groups with offset management
- Real-time streaming with gRPC
- Batch operations for performance
- Pattern-based operations
- Comprehensive statistics

This provides **full Redis + Kafka + RabbitMQ functionality** in 38 unified APIs! ðŸš€

---

## ðŸ“¬ Queue APIs

### Basic Queue Operations

#### Go Client - `client.Queue.*`
```go
// PUSH - Add message to queue
msgID, err := client.Queue.Push(ctx, "notifications", []byte("Welcome email"), 0)

// PUSH with delay
msgID, err := client.Queue.Push(ctx, "delayed_jobs", []byte("process later"), 30*time.Second)

// POP - Remove and return message (blocking)
message, err := client.Queue.Pop(ctx, "notifications", 5*time.Second)

// PEEK - View next message without removing
message, err := client.Queue.Peek(ctx, "notifications")

// SIZE - Get queue length
size, err := client.Queue.Size(ctx, "notifications")

// PURGE - Remove all messages
purged, err := client.Queue.Purge(ctx, "notifications")
```

#### Node.js Client - `client.queue.*`
```javascript
// PUSH - Add message to queue
const msgId = await client.queue.push('notifications', Buffer.from('Welcome email'));

// PUSH with delay
const msgId = await client.queue.push('delayed_jobs', Buffer.from('process later'), 30);

// POP - Remove and return message (blocking)
const message = await client.queue.pop('notifications', 5);

// PEEK - View next message without removing
const message = await client.queue.peek('notifications');

// SIZE - Get queue length
const size = await client.queue.size('notifications');

// PURGE - Remove all messages
const purged = await client.queue.purge('notifications');
```

### Message Acknowledgment

#### Go Client
```go
// ACK - Acknowledge message processing
err = client.Queue.Ack(ctx, "notifications", messageID)

// NACK - Negative acknowledgment (requeue)
err = client.Queue.Nack(ctx, "notifications", messageID, true) // requeue=true

// REJECT - Reject message (dead letter)
err = client.Queue.Reject(ctx, "notifications", messageID)

// REQUEUE - Put message back in queue
err = client.Queue.Requeue(ctx, "notifications", messageID, 10*time.Second) // delay
```

#### Node.js Client
```javascript
// ACK - Acknowledge message processing
await client.queue.ack('notifications', messageId);

// NACK - Negative acknowledgment (requeue)
await client.queue.nack('notifications', messageId, true); // requeue=true

// REJECT - Reject message (dead letter)
await client.queue.reject('notifications', messageId);

// REQUEUE - Put message back in queue
await client.queue.requeue('notifications', messageId, 10); // delay in seconds
```

### Batch Queue Operations

#### Go Client
```go
// PUSH_BATCH - Add multiple messages
messages := [][]byte{
    []byte("msg1"),
    []byte("msg2"),
    []byte("msg3"),
}
delays := []time.Duration{0, 5*time.Second, 10*time.Second}
msgIDs, err := client.Queue.PushBatch(ctx, "bulk_jobs", messages, delays)

// POP_BATCH - Get multiple messages
messages, err := client.Queue.PopBatch(ctx, "bulk_jobs", 10, 5*time.Second)

// ACK_BATCH - Acknowledge multiple messages
err = client.Queue.AckBatch(ctx, "bulk_jobs", msgIDs)
```

#### Node.js Client
```javascript
// PUSH_BATCH - Add multiple messages
const messages = [Buffer.from('msg1'), Buffer.from('msg2'), Buffer.from('msg3')];
const delays = [0, 5, 10];
const msgIds = await client.queue.pushBatch('bulk_jobs', messages, delays);

// POP_BATCH - Get multiple messages
const messages = await client.queue.popBatch('bulk_jobs', 10, 5);

// ACK_BATCH - Acknowledge multiple messages
await client.queue.ackBatch('bulk_jobs', msgIds);
```

### Queue Management

#### Go Client
```go
// STATS - Get queue statistics
stats, err := client.Queue.Stats(ctx, "notifications")
// Returns: Name, Size, Processed, Failed, Consumers, DeadLetters

// LIST_QUEUES - Get all queue names
queues, err := client.Queue.ListQueues(ctx)

// CREATE_QUEUE - Create queue with options
options := &gomsg.QueueOptions{
    MaxRetries:    3,
    RetryDelay:    30 * time.Second,
    DeadLetterTTL: 24 * time.Hour,
}
err = client.Queue.CreateQueue(ctx, "priority_jobs", options)

// DELETE_QUEUE - Remove queue and all messages
err = client.Queue.DeleteQueue(ctx, "old_queue")
```

#### Node.js Client
```javascript
// STATS - Get queue statistics
const stats = await client.queue.stats('notifications');

// LIST_QUEUES - Get all queue names
const queues = await client.queue.listQueues();

// CREATE_QUEUE - Create queue with options
const options = {
    maxRetries: 3,
    retryDelay: 30,
    deadLetterTTL: 86400
};
await client.queue.createQueue('priority_jobs', options);

// DELETE_QUEUE - Remove queue and all messages
await client.queue.deleteQueue('old_queue');
```

---

## ðŸŒŠ Stream APIs

### Topic Management

#### Go Client - `client.Stream.*`
```go
// CREATE_TOPIC - Create stream topic
err = client.Stream.CreateTopic(ctx, "user_events", 4) // 4 partitions

// DELETE_TOPIC - Remove topic
err = client.Stream.DeleteTopic(ctx, "old_events")

// LIST_TOPICS - Get all topics
topics, err := client.Stream.ListTopics(ctx)

// DESCRIBE_TOPIC - Get topic details
info, err := client.Stream.DescribeTopic(ctx, "user_events")
// Returns: Name, Partitions, ReplicationFactor, Config

// PURGE - Remove all messages from topic
purged, err := client.Stream.Purge(ctx, "user_events")
```

#### Node.js Client - `client.stream.*`
```javascript
// CREATE_TOPIC - Create stream topic
await client.stream.createTopic('user_events', 4);

// DELETE_TOPIC - Remove topic
await client.stream.deleteTopic('old_events');

// LIST_TOPICS - Get all topics
const topics = await client.stream.listTopics();

// DESCRIBE_TOPIC - Get topic details
const info = await client.stream.describeTopic('user_events');

// PURGE - Remove all messages from topic
const purged = await client.stream.purge('user_events');
```

### Message Publishing

#### Go Client
```go
// PUBLISH - Send message to stream
headers := map[string]string{"source": "web", "version": "1.0"}
msg, err := client.Stream.Publish(ctx, "user_events", "user:123", []byte(`{"action":"login"}`), headers)

// PUBLISH_BATCH - Send multiple messages
messages := []*gomsg.StreamMessage{
    {PartitionKey: "user:1", Data: []byte(`{"action":"login"}`)},
    {PartitionKey: "user:2", Data: []byte(`{"action":"logout"}`)},
}
results, err := client.Stream.PublishBatch(ctx, "user_events", messages)

// PUBLISH_ASYNC - Non-blocking publish
future := client.Stream.PublishAsync(ctx, "user_events", "user:123", []byte(`{"action":"click"}`), nil)
// Handle result later: msg, err := future.Get()
```

#### Node.js Client
```javascript
// PUBLISH - Send message to stream
const headers = {source: 'web', version: '1.0'};
const msg = await client.stream.publish('user_events', 'user:123', Buffer.from('{"action":"login"}'), headers);

// PUBLISH_BATCH - Send multiple messages
const messages = [
    {partitionKey: 'user:1', data: Buffer.from('{"action":"login"}')},
    {partitionKey: 'user:2', data: Buffer.from('{"action":"logout"}')}
];
const results = await client.stream.publishBatch('user_events', messages);

// PUBLISH_ASYNC - Non-blocking publish
const promise = client.stream.publishAsync('user_events', 'user:123', Buffer.from('{"action":"click"}'));
```

### Message Consumption

#### Go Client
```go
// READ - Read messages from offset
messages, err := client.Stream.Read(ctx, "user_events", 0, 100, 10) // partition, offset, limit

// READ_FROM - Read messages from timestamp
fromTime := time.Now().Add(-1 * time.Hour)
messages, err := client.Stream.ReadFrom(ctx, "user_events", 0, fromTime, 10)

// READ_LATEST - Read most recent messages
messages, err := client.Stream.ReadLatest(ctx, "user_events", 0, 10)

// SUBSCRIBE - Real-time message consumption
msgChan, errChan, err := client.Stream.Subscribe(ctx, "user_events", 0, 100)
for msg := range msgChan {
    // Process message
    fmt.Printf("Received: %s\n", string(msg.Data))
}
```

#### Node.js Client
```javascript
// READ - Read messages from offset
const messages = await client.stream.read('user_events', 0, 100, 10);

// READ_FROM - Read messages from timestamp
const fromTime = new Date(Date.now() - 3600000); // 1 hour ago
const messages = await client.stream.readFrom('user_events', 0, fromTime, 10);

// READ_LATEST - Read most recent messages
const messages = await client.stream.readLatest('user_events', 0, 10);

// SUBSCRIBE - Real-time message consumption
const subscription = await client.stream.subscribe('user_events', 0, 100);
subscription.on('message', (msg) => {
    console.log('Received:', msg.data.toString());
});
```

### Consumer Groups

#### Go Client
```go
// SUBSCRIBE_GROUP - Join consumer group
msgChan, errChan, err := client.Stream.SubscribeGroup(ctx, "user_events", "analytics_group", "consumer_1")

// COMMIT_OFFSET - Commit processed offset
err = client.Stream.CommitGroupOffset(ctx, "user_events", "analytics_group", 0, 150)

// GET_OFFSET - Get current group offset
offset, err := client.Stream.GetGroupOffset(ctx, "user_events", "analytics_group", 0)

// LIST_GROUPS - Get all consumer groups
groups, err := client.Stream.ListConsumerGroups(ctx, "user_events")

// DESCRIBE_GROUP - Get group details
info, err := client.Stream.DescribeConsumerGroup(ctx, "user_events", "analytics_group")
// Returns: GroupID, Members, Offsets, Lag

// RESET_GROUP - Reset group to specific offset
err = client.Stream.ResetConsumerGroup(ctx, "user_events", "analytics_group", 0, 0)
```

#### Node.js Client
```javascript
// SUBSCRIBE_GROUP - Join consumer group
const groupSub = await client.stream.subscribeGroup('user_events', 'analytics_group', 'consumer_1');

// COMMIT_OFFSET - Commit processed offset
await client.stream.commitGroupOffset('user_events', 'analytics_group', 0, 150);

// GET_OFFSET - Get current group offset
const offset = await client.stream.getGroupOffset('user_events', 'analytics_group', 0);

// LIST_GROUPS - Get all consumer groups
const groups = await client.stream.listConsumerGroups('user_events');

// DESCRIBE_GROUP - Get group details
const info = await client.stream.describeConsumerGroup('user_events', 'analytics_group');

// RESET_GROUP - Reset group to specific offset
await client.stream.resetConsumerGroup('user_events', 'analytics_group', 0, 0);
```

### Stream Management

#### Go Client
```go
// GET_PARTITION_INFO - Get partition details
info, err := client.Stream.GetPartitionInfo(ctx, "user_events", 0)
// Returns: Partition, StartOffset, EndOffset, MessageCount

// SEEK - Move consumer to specific offset
err = client.Stream.Seek(ctx, "user_events", 0, 100)

// GET_OFFSETS - Get partition offset range
start, end, err := client.Stream.GetOffsets(ctx, "user_events", 0)

// COMPACT - Trigger log compaction
err = client.Stream.Compact(ctx, "user_events")
```

#### Node.js Client
```javascript
// GET_PARTITION_INFO - Get partition details
const info = await client.stream.getPartitionInfo('user_events', 0);

// SEEK - Move consumer to specific offset
await client.stream.seek('user_events', 0, 100);

// GET_OFFSETS - Get partition offset range
const offsets = await client.stream.getOffsets('user_events', 0); // {start, end}

// COMPACT - Trigger log compaction
await client.stream.compact('user_events');
```

---

## ðŸ“Š Complete API Summary

| **Category** | **Go APIs** | **Node.js APIs** | **Total Operations** |
|--------------|-------------|------------------|---------------------|
| **Connection** | 3 methods | 3 methods | **6** |
| **KV Basic** | 7 methods | 7 methods | **14** |
| **KV Batch** | 3 methods | 3 methods | **6** |
| **KV Atomic** | 4 methods | 4 methods | **8** |
| **KV Pattern** | 4 methods | 4 methods | **8** |
| **Queue Basic** | 6 methods | 6 methods | **12** |
| **Queue ACK/NACK** | 4 methods | 4 methods | **8** |
| **Queue Batch** | 3 methods | 3 methods | **6** |
| **Queue Management** | 4 methods | 4 methods | **8** |
| **Stream Topics** | 5 methods | 5 methods | **10** |
| **Stream Publishing** | 3 methods | 3 methods | **6** |
| **Stream Consumption** | 4 methods | 4 methods | **8** |
| **Stream Groups** | 6 methods | 6 methods | **12** |
| **Stream Management** | 4 methods | 4 methods | **8** |
| **TOTAL** | **60 APIs** | **60 APIs** | **120 APIs** |

---

## ðŸš€ Usage Examples

### Complete KV Example
```go
// Go - Complete KV workflow
client.KV.Set(ctx, "user:123", []byte("John"), 5*time.Minute)
client.KV.Increment(ctx, "user:123:views", 1)
keys, _ := client.KV.Keys(ctx, "user:*")
client.KV.MGet(ctx, keys)
```

### Complete Queue Example
```go
// Go - Complete Queue workflow with ACK/NACK
msgID, _ := client.Queue.Push(ctx, "jobs", []byte("process-image"))
msg, _ := client.Queue.Pop(ctx, "jobs", 30*time.Second)
// Process message...
if success {
    client.Queue.Ack(ctx, "jobs", msg.ID)
} else {
    client.Queue.Nack(ctx, "jobs", msg.ID, true) // requeue
}
```

### Complete Stream Example
```go
// Go - Complete Stream workflow with consumer groups
client.Stream.CreateTopic(ctx, "events", 4)
client.Stream.Publish(ctx, "events", "user:1", []byte(`{"action":"login"}`), nil)

// Consumer group
msgChan, _, _ := client.Stream.SubscribeGroup(ctx, "events", "processors", "worker-1")
for msg := range msgChan {
    // Process message...
    client.Stream.CommitGroupOffset(ctx, "events", "processors", msg.Partition, msg.Offset)
}
```

This comprehensive API provides **enterprise-grade functionality** comparable to Redis + Kafka + RabbitMQ combined! ðŸŽ¯
